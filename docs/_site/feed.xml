<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-07-02T16:38:41+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/feed.xml</id><title type="html">Sakhineti Praveena | JdeRobot x GSoC2025</title><subtitle>Sakhineti Praveena | JdeRobot x GSoC2025
</subtitle><entry><title type="html">Week 4 (Jun 23 - Jun 29)</title><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-4/" rel="alternate" type="text/html" title="Week 4 (Jun 23 - Jun 29)" /><published>2025-06-30T00:00:00+05:30</published><updated>2025-06-30T00:00:00+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-4</id><content type="html" xml:base="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-4/"><![CDATA[<h2 id="meeting-with-mentors">Meeting with Mentors</h2>

<ul>
  <li>This week’s meeting began with an update on my progress. I shared details about the <code class="language-plaintext highlighter-rouge">DetectionMetricsFactory</code> class I created. David suggested that a common base class could be used, from which both <code class="language-plaintext highlighter-rouge">SegmentationMetricsFactory</code> and <code class="language-plaintext highlighter-rouge">DetectionMetricsFactory</code> can be derived.</li>
  <li>We discussed whether to use Streamlit or Gradio for the UI. The consensus was that the choice depends on our end goal. Sergio clarified that the goal is to build a full application (not just a demo), so we decided to proceed with Streamlit. This will allow us to support future UI advancements more easily.</li>
  <li>I asked the mentors about their idea for the UI design. They shared details and references from the UI used in v1, which I will use as inspiration.</li>
  <li>Finally, I discussed an error I encountered while implementing the example notebook. The issue was due to an improperly defined ontology translation. David provided helpful suggestions on how to define the ontology correctly.</li>
</ul>

<hr />

<h2 id="to-do-for-this-week">To Do for This Week</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Test and fix all issues in the code developed so far.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />(Partially done for small dataset) Have a working example for DetectionMetrics using COCO and a Torch model.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Design the basic UI and get feedback from mentors.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Get familiar with Streamlit to kickstart UI development.</li>
</ul>

<hr />

<h2 id="progress">Progress</h2>

<ul>
  <li>I started by working on the example notebook for detection metrics. The <code class="language-plaintext highlighter-rouge">KeyError</code> I faced last week was due to ontology translation issues and because the model’s integer output wasn’t converted to a string before being used as a key. This is now resolved.</li>
  <li>I restructured <code class="language-plaintext highlighter-rouge">coco.py</code> and <code class="language-plaintext highlighter-rouge">torch_detection.py</code> further after comparing them to the segmentation pipeline, aiming for consistency between both pipelines.</li>
  <li>
    <p>The example notebook now runs fine. However, when loading larger datasets, I encountered the error:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'[srcBuf length] &gt; 0 INTERNAL ASSERT FAILED at "/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/OperationUtils.mm":341, please report a bug to PyTorch. Placeholder tensor is empty!'
</code></pre></div>    </div>

    <p>I tried to debug this by ensuring empty images aren’t loaded into the model, but the error persisted. I’m unsure if this is a model or MPS issue. Further investigation was blocked by kernel crashes. Resolving these MPS issues is still pending, but I was able to run the notebook and get results for a small dataset of 5 images.</p>
  </li>
  <li>Next, I explored UI design. I started by building a Figma design for the UI: <a href="https://www.figma.com/design/UWvShVroVUUEfD3dEdwxEy/Detection-metrics?node-id=0-1&amp;t=xY40vfWrA6dDT2WE-1">DetectionMetrics Figma</a></li>
  <li>I thought having <strong>Viewer</strong>, <strong>Detector</strong>, and <strong>Evaluator</strong> tabs in the sidebar would be relevant (similar to previous versions). Each tab will support both segmentation and detection. In Viewer, users can upload datasets and view sample images with ground truth masks/bounding boxes. In Detector, users can get inference for segmentation/detection. In Evaluator, all metrics will be displayed in the UI.</li>
</ul>
<div style="text-align: center;">
<img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-06-30-Week-4/figma.png" alt="Old Class Structure" style="width: 95%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);" />
</div>
<ul>
  <li>I then began learning how Streamlit works. I created a demo app: <a href="https://github.com/TheRoboticsClub/gsoc2025-Sakhineti_Praveena/tree/main/demos/streamlit_demo.py"><code class="language-plaintext highlighter-rouge">streamlit_demo.py</code></a>. The app features three sections in the sidebar. In the dataset tab, users can upload a sample image and select the annotation. On clicking <strong>View Sample</strong>, the image with bounding boxes is displayed in the UI.</li>
</ul>
<div style="text-align: center;">
<img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-06-30-Week-4/demo.png" alt="Old Class Structure" style="width: 95%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);" />
</div>

<hr />

<h3 id="whats-next">What’s Next?</h3>
<p>My main goal for next week is to get the draft PR merged. I will thoroughly test and fix all bugs, and clean up the codebase before doing that. And I also plan to make significant progress on the UI and get the basic implementation done.</p>]]></content><author><name>Sakhineti Praveena</name></author><category term="blog" /><summary type="html"><![CDATA[Meeting with Mentors]]></summary></entry><entry><title type="html">Week 3 (Jun 16 - Jun 22)</title><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-3/" rel="alternate" type="text/html" title="Week 3 (Jun 16 - Jun 22)" /><published>2025-06-23T00:00:00+05:30</published><updated>2025-06-23T00:00:00+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-3</id><content type="html" xml:base="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-3/"><![CDATA[<h2 id="meeting-with-mentors">Meeting with Mentors</h2>

<p>In this week’s meeting, I began by sharing an update on my current progress. The mentors gave positive feedback and helped clarify the utility functions used in <code class="language-plaintext highlighter-rouge">torch.py</code> for segmentation. David recommended moving some of these utilities to a shared <code class="language-plaintext highlighter-rouge">torch_utils.py</code>, as they are commonly used across both modules.</p>

<p>We also discussed the next set of deliverables and finalized this week’s goals. Sergio raised a thoughtful question about our choice of <strong>Streamlit</strong> for the UI. He suggested exploring <strong>Gradio</strong> too as an alternative, especially since it is part of hugging face we would have better support. Based on this, I decided to compare both frameworks and evaluate which suits our needs better.</p>

<hr />

<h2 id="to-do-for-the-week">To-Do for the Week</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Finish support for Torch image detection models</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Add metrics for detection model evaluation</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Create an example notebook for detection metrics evaluation using COCO and Torch models</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Compare Streamlit and Gradio for building the user interface</li>
</ul>

<hr />

<h2 id="progress">Progress</h2>

<ul>
  <li>
    <p>I began by finishing the <code class="language-plaintext highlighter-rouge">TorchImageDetectionModel</code> class where I implemented the <code class="language-plaintext highlighter-rouge">inference()</code>, <code class="language-plaintext highlighter-rouge">eval()</code>, and <code class="language-plaintext highlighter-rouge">computation_cost()</code> methods for Torch-based object detection models.</p>
  </li>
  <li>
    <p>For detection metrics, I chose to define a new class, instead of extending the existing <code class="language-plaintext highlighter-rouge">MetricsFactory</code> meant for segmentation. I added a <code class="language-plaintext highlighter-rouge">DetectionMetricsFactory</code> in <code class="language-plaintext highlighter-rouge">detection_metrics.py</code>.<br />
It computes per-class metrics (AP, Precision, Recall, TP, FP and FN) and returns them in a pandas DataFrame. As the last row I have also included mAP in the data frame before returning it. It should look like so :</p>
  </li>
</ul>

<div style="text-align: center;">
<img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-06-23-Week-3/dataframe.png" alt="Old Class Structure" style="width: 95%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);" />
</div>

<ul>
  <li>
    <p>I started creating a sample notebook to test my changes using COCO and a Torch model. Since this is the first time I am running my new changes end-to-end, I was facing minor errors. I started debugging each one and made the necessary changes to the classes. I am currently stuck with an error with respect to the indices mismatching between coco and torch. I will take the mentors help with this in the meeting today.</p>
  </li>
  <li>
    <p>Simultaneously, I began comparing <strong>Gradio</strong> and <strong>Streamlit</strong> to help decide which framework to use for our UI. I have listed down the pros and cons for each below.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Gradio
  Pros
  -  Built-in Hosting &amp; Sharing (Gradio Hub, HF Spaces)  
  -  Easy Jupyter Notebook Integration  
  -  Seamless Hugging Face, PyTorch, TensorFlow, sklearn integration  
  -  Clean UI for Inputs/Outputs  
  -  Embeddable in websites via `iframe`

  Cons
  -  Limited Layout Customization  
  -  Multi-page support is basic  
  -  No Built-in Session State  
  -  Not suitable for dashboards or long-running processes  
  -  Limited styling/theming support  

Streamlit
  Pros
  -  Ideal for Dashboards, Visualizations, Analytics  
  -  Native Multi-page App Support  
  -  Advanced Layouts (columns, tabs, sidebars)  
  -  Built-in Session State  
  -  Broad widget &amp; charting support (Altair, Plotly, etc.)  
  -  Better for scalable, real-time apps  

  Cons
  -  Slightly higher learning curve  
  -  No one-click public sharing from notebooks  
  -  Not embeddable in Jupyter  
  -  No native model integration (manual setup needed)
</code></pre></div>    </div>

    <p>These are the blogs I referred to <a href="https://uibakery.io/blog/streamlit-vs-gradio?utm_source=chatgpt.com">uibakery.io blog</a>, <a href="https://blog.usro.net/2024/10/gradio-vs-streamlit/?utm_source=chatgpt.com">usro.net blog</a> and <a href="https://myscale.com/blog/streamlit-vs-gradio-ultimate-showdown-python-dashboards/?utm_source=chatgpt.com">myscale.com blog</a></p>
  </li>
</ul>

<hr />

<h3 id="whats-next">What’s Next?</h3>

<p>For the upcoming week, I’ll work on testing my code changes end-to-end. Fix any errors I face along the way. I will aim to finish the example notebook I started working on. And based on the suggestion from the mentors regarding which UI framework to use, I’ll also start prototyping the user interface.</p>]]></content><author><name>Sakhineti Praveena</name></author><category term="blog" /><summary type="html"><![CDATA[Meeting with Mentors]]></summary></entry><entry><title type="html">Week 2 (Jun 09 - Jun 15)</title><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-2/" rel="alternate" type="text/html" title="Week 2 (Jun 09 - Jun 15)" /><published>2025-06-16T00:00:00+05:30</published><updated>2025-06-16T00:00:00+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-2</id><content type="html" xml:base="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-2/"><![CDATA[<h2 id="meeting-with-mentors">Meeting with Mentors</h2>

<p>We discussed about the progress made in the previous week where I have refactored dataset and model classes. I also raised an issue I encountered during testing: in one of the examples, the inference output appeared distorted compared to earlier results. David mentioned he would look into it on his end(which was fixed later that week through <a href="https://github.com/JdeRobot/DetectionMetrics/pull/314">Issue #314</a>).</p>

<p>Another topic was the current file structure. Right now, both the <code class="language-plaintext highlighter-rouge">dataset</code> and <code class="language-plaintext highlighter-rouge">model</code> directories have a lot of files, and David suggested we may consider restructuring them later as the project evolves. Overall, the feedback from mentors was positive, and we moved ahead with planning goals for the coming week.</p>

<hr />

<h2 id="this-weeks-to-do">This Week’s To-Do</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Add support for COCO dataset</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add support for Torch detection models (Partially done)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explore Streamlit if time permits</li>
</ul>

<hr />

<h2 id="progress">Progress</h2>

<h4 id="coco-dataset-support">COCO Dataset Support</h4>

<ul>
  <li>I started by creating <code class="language-plaintext highlighter-rouge">coco.py</code>, which includes a <code class="language-plaintext highlighter-rouge">CocoDataset</code> class derived from <code class="language-plaintext highlighter-rouge">ImageDetectionDataset</code>.</li>
  <li>I was spending some time to check if I should use pycocotools or write our own parsing logic.  Some GitHub <a href="https://github.com/cocodataset/cocoapi/issues/169">issues</a> pointed out that the official <code class="language-plaintext highlighter-rouge">pycocotools</code> is not Windows-friendly.</li>
  <li>We found a <a href="https://github.com/cocodataset/cocoapi/issues/415#issuecomment-627313816">workaround</a> using <code class="language-plaintext highlighter-rouge">pycocotools-windows</code> for Windows machines. After discussing with mentors, we decided to handle installation conditionally. David also confirmed by installing the library in his windows machine. I have handled the installation in pyproject.toml like shown in the code below using this snippet:</li>
</ul>

<pre> pycocotools = { version = "^2.0.7", markers = "sys_platform != 'win32'" }
pycocotools-windows = { version = "^2.0.0.2", markers = "sys_platform == 'win32'" }</pre>

<ul>
  <li>Within the <code class="language-plaintext highlighter-rouge">CocoDataset</code> class:
    <ul>
      <li>_build_ontology() maps COCO category IDs to the framework’s format.</li>
      <li>_build_dataframe() prepares the structure used by the base class.</li>
      <li>read_annotation() processes annotations for each image based on image_id.</li>
    </ul>
  </li>
</ul>

<h4 id="torch-model-refactor">Torch Model Refactor</h4>

<ul>
  <li>I renamed <code class="language-plaintext highlighter-rouge">torch.py</code> to <code class="language-plaintext highlighter-rouge">torch_segmentation.py</code> and updated the imports across all example scripts accordingly.</li>
  <li>Then I created <code class="language-plaintext highlighter-rouge">torch_detection.py</code> to support detection models.</li>
  <li>Taking <code class="language-plaintext highlighter-rouge">torch_segmentation.py</code> as a reference, I modified:
    <ul>
      <li>data_to_device()</li>
      <li>get_data_shape()</li>
      <li>get_computational_cost()</li>
    </ul>

    <p>to account for the different input format used in detection: <code class="language-plaintext highlighter-rouge">List[Dict[str, Tensor]]</code>.</p>
  </li>
  <li>I wasn’t sure if unsqueeze_data() and the CustomResize class were necessary for detection, so I skipped them for now.</li>
  <li>I was unwell on Tuesday and Wednesday and couldn’t make as much progress as I had planned. I couldn’t fully finish torch_detection.py this week. However, I’ve committed the partial changes in the <a href="https://github.com/JdeRobot/DetectionMetrics/pull/312">draft PR #312</a>.</li>
</ul>

<hr />

<h2 id="going-ahead">Going ahead</h2>

<p>Next week, I’ll aim to:</p>
<ul>
  <li>Complete the Torch detection model integration</li>
  <li>Add the metrics evaluation part for detection</li>
  <li>Add an end-to-end example with COCO + Torch detection</li>
</ul>]]></content><author><name>Sakhineti Praveena</name></author><category term="blog" /><summary type="html"><![CDATA[Meeting with Mentors]]></summary></entry><entry><title type="html">Week 1 (Jun 01 - Jun 08)</title><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-1/" rel="alternate" type="text/html" title="Week 1 (Jun 01 - Jun 08)" /><published>2025-06-09T00:00:00+05:30</published><updated>2025-06-09T00:00:00+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-1</id><content type="html" xml:base="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Week-1/"><![CDATA[<h2 id="meeting-with-mentors">Meeting with Mentors</h2>

<p>This week, we discussed the implementation plan in more detail. David suggested that I start with refactoring the <code class="language-plaintext highlighter-rouge">dataset.py</code> module as a solid entry point into the codebase. Once that is complete, I can gradually move on to refactoring the model class and metrics module.</p>

<p>They also encouraged me to open a draft PR early so that my progress can be continuously reviewed and improved. David also mentioned an exciting use case for our tool: it will be used to evaluate a trash detection model, making the detection metrics we build directly impactful.</p>

<hr />

<h2 id="this-weeks-to-do">This Week’s To-Do</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Refactor dataset.py</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Refactor model.py</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explore Streamlit (if time permits)</li>
</ul>

<hr />

<h2 id="progress">Progress</h2>

<p>I started this week by refactoring the dataset classes. I followed the structure David and I had discussed during the application stage.</p>

<p>The older class structure looked like this:</p>

<div style="text-align: center;">
  <img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-06-09-Week-1/oldclassstructure.png" alt="Old Class Structure" style="width: 50%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);" />
</div>

<p>To support detection models more effectively, I’ve reorganized the class hierarchy:</p>

<div style="text-align: center;">
  <img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-06-09-Week-1/newclassstructure.png" alt="Old Class Structure" style="width: 50%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);" />
</div>

<p>I began by identifying the common functionalities shared between segmentation and detection tasks. These were moved into a new base class, <code class="language-plaintext highlighter-rouge">PerceptionDataset</code>. In this class, I’ve implemented methods like: <code class="language-plaintext highlighter-rouge">__init__</code> , <code class="language-plaintext highlighter-rouge">__len__</code> , <code class="language-plaintext highlighter-rouge">make_fname_global</code> and <code class="language-plaintext highlighter-rouge">append</code> (assuming the ontology structure is the same as segmentation for now)</p>

<p>If there is some extra info per class in the datasets, the logic in <code class="language-plaintext highlighter-rouge">append</code> can be overridden later in the <code class="language-plaintext highlighter-rouge">DetectionDataset</code> class.</p>

<p>Next, I defined the functions specific to segmentation and detection within their respective subclasses. I also incorporated some of the model refactoring work I had prepared during the application phase and finished up the changes.</p>

<p>I’ve raised a <a href="https://github.com/JdeRobot/DetectionMetrics/pull/312"><strong>draft PR</strong></a> that includes all these changes. Following David’s suggestion, I split the monolithic class into multiple files to improve modularity and maintainability.</p>
<div style="text-align: center;">
  <img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-06-09-Week-1/filestructure.png" alt="Old Class Structure" style="width: 50%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);" />
</div>

<hr />

<h2 id="going-ahead">Going ahead</h2>

<p>I plan to thoroughly test all the changes I have made. If everything is alright I am thinking defining coco.py and pascalvoc.py could be a potential next step. I also didn’t get much time to explore streamlit this week, hoping to do that as well this coming week.</p>]]></content><author><name>Sakhineti Praveena</name></author><category term="blog" /><summary type="html"><![CDATA[Meeting with Mentors]]></summary></entry><entry><title type="html">Community Bonding Week-2</title><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Community-Bonding-Week-2/" rel="alternate" type="text/html" title="Community Bonding Week-2" /><published>2025-05-30T00:00:00+05:30</published><updated>2025-05-30T00:00:00+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Community-Bonding-Week-2</id><content type="html" xml:base="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Community-Bonding-Week-2/"><![CDATA[<h2 id="meeting-with-mentors">Meeting with Mentors</h2>

<p>Started this week with the usual weekly sync up with the mentors, where we discussed various aspects of the project and my progress so far.  David encouraged me to play around a bit with the datasets, download, parse and implement inference, even if it’s simple at this stage.</p>

<p>Sergio highlighted the importance of understanding the input and output formats of the models, and how crucial it is to be able to handle these effectively to extract meaningful metrics. He pointed out that knowing the format of the dataset and how they are passed to the models are crucial for the project over internal workings of each model layer. Santiago shared a helpful insight that any complex computer vision problem can be broken down into smaller subproblems of classification or regression. And finally, david addressed the elephant in the room — we all learned how to pronounce each other’s names correctly yay!</p>

<h2 id="this-weeks-to-do">This Week’s To-Do</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Go through different dataset formats.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Download the datasets and parse them.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Use a portion of a COCO dataset to implement inference</li>
</ul>

<h2 id="progress">Progress</h2>

<p>This week was all about exploring dataset formats and getting hands-on with them. I started with the COCO dataset and downloaded the 2017 validation images, training images, and the 2017 Train/Val annotations from the <a href="https://cocodataset.org/#download">official COCO website</a>. My goal was to understand how the dataset is structured, how the annotations are stored, and how to work with them effectively.</p>

<p>I referred to the <a href="https://cocodataset.org/#format-data">COCO data format documentation</a> to dive deeper into the annotation structure. The main <code class="language-plaintext highlighter-rouge">annotations.json</code> file contains several top-level keys: <code class="language-plaintext highlighter-rouge">info</code>, <code class="language-plaintext highlighter-rouge">licenses</code>, <code class="language-plaintext highlighter-rouge">images</code>, <code class="language-plaintext highlighter-rouge">annotations</code>, and <code class="language-plaintext highlighter-rouge">categories</code>. Each object in the <code class="language-plaintext highlighter-rouge">annotations</code> list typically includes fields such as <code class="language-plaintext highlighter-rouge">id</code>, <code class="language-plaintext highlighter-rouge">image_id</code>, <code class="language-plaintext highlighter-rouge">category_id</code>, <code class="language-plaintext highlighter-rouge">bbox</code>, <code class="language-plaintext highlighter-rouge">area</code>, and <code class="language-plaintext highlighter-rouge">iscrowd</code>. The <code class="language-plaintext highlighter-rouge">bbox</code> is formatted as <code class="language-plaintext highlighter-rouge">[x, y, width, height]</code>, where <code class="language-plaintext highlighter-rouge">(x, y)</code> marks the top-left corner of the bounding box. To understand this better, I started mapping the annotations to their corresponding images and visualizing them with bounding boxes in a notebook called <a href="https://github.com/TheRoboticsClub/gsoc2025-Sakhineti_Praveena/tree/main/demos/coco.ipynb">coco.ipynb</a>.</p>

<p>Initially, I redundantly mapped the annotations to images in <a href="https://github.com/TheRoboticsClub/gsoc2025-Sakhineti_Praveena/tree/main/demos/coco.ipynb">coco.ipynb</a>. But later, I discovered the official COCO API, which simplifies the process of loading, parsing, and visualizing annotations. It was a bit humbling to realize how much easier things could have been — but also a valuable lesson in finding the right tools. With this understanding, I proceeded to implement a simple object detection pipeline <a href="https://github.com/TheRoboticsClub/gsoc2025-Sakhineti_Praveena/tree/main/demos/detection_coco.ipynb">detection_coco.ipynb</a>, using a pretrained Faster R-CNN model from TorchVision. I ran inference on a small subset of 10 images from the COCO dataset to test how everything worked together, below is an image I populated inference for.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/objDetection-480.webp 480w,/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/objDetection-800.webp 800w,/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/objDetection-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/objDetection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

<p>Following that, I explored the Pascal VOC dataset. I downloaded the 2012 training and validation sets from the <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">official VOC site</a>. Unlike COCO, VOC uses separate XML files for each image to store annotations. Each file contains object instances with tags like <code class="language-plaintext highlighter-rouge">&lt;name&gt;</code> for the class label, <code class="language-plaintext highlighter-rouge">&lt;difficult&gt;</code> (a flag to indicate whether the object should be considered in evaluation), and <code class="language-plaintext highlighter-rouge">&lt;bndbox&gt;</code>, which specifies the bounding box coordinates in the format <code class="language-plaintext highlighter-rouge">(xmin, ymin, xmax, ymax)</code>. I parsed and visualized several VOC images in a separate notebook  <a href="https://github.com/TheRoboticsClub/gsoc2025-Sakhineti_Praveena/tree/main/demos/PascalVoc.ipynb">PascalVoc.ipynb</a>. I also came across a useful <a href="https://github.com/shiyemin/voc2coco/blob/master/voc2coco.py">script</a> to convert VOC annotations to COCO format.</p>

<p>In addition, I briefly explored the YOLO dataset format. YOLO annotations are stored in <code class="language-plaintext highlighter-rouge">.txt</code> files, one per image, with each line containing five values: <code class="language-plaintext highlighter-rouge">&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code>, all normalized between 0 and 1 relative to the image dimensions. The format is simple and optimized for real-time detection.</p>

<p>Finally, towards the end of the week, I began exploring <code class="language-plaintext highlighter-rouge">Streamlit</code>, a lightweight framework for building data apps in Python. Although I didn’t get the chance to complete a working GUI yet, I managed to grasp the basics — which I plan to build on in the coming weeks as I move toward creating a user-friendly interface for visualizing model outputs.</p>

<h2 id="gsoc25-jderobot-kickoff-meeting-2905">GSoC’25 JdeRobot Kickoff Meeting (29/05)</h2>

<p>This week, I had the GSoC’25 kickoff meeting for JdeRobot, where I got to meet my fellow contributors, mentors, and the org admins. Pedro Arias began the session with a quick overview of JdeRobot, highlighting its main focus areas — robotics education, AI-driven robotics, and robot programming tools. It was inspiring to see how wide-ranging and impactful the organization’s work is.</p>

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/kickoffmeeting-480.webp 480w,/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/kickoffmeeting-800.webp 800w,/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/kickoffmeeting-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/gsoc2025-Sakhineti_Praveena/assets/img/blog/2025-05-30-Community-Bonding-Week-2/kickoffmeeting.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

<p>We then had a round of introductions, which was a great way to connect with like-minded people working on exciting and diverse projects. Following that, Pedro shared some helpful insights into the work methodologies we’ll be following, including contribution guidelines and evaluation timelines. David also gave us an important reminder to be proactive, not just during our weekly check-ins but throughout the week — by sharing timely updates, asking questions, and staying engaged. I will make sure to keep that in mind. He reassured us that no question is too small or silly. Overall it was a really helpful session to kick things off!</p>]]></content><author><name>Sakhineti Praveena</name></author><category term="blog" /><summary type="html"><![CDATA[Meeting with Mentors]]></summary></entry><entry><title type="html">Community Bonding Week-1</title><link href="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Community-Bonding-Week-1/" rel="alternate" type="text/html" title="Community Bonding Week-1" /><published>2025-05-23T00:00:00+05:30</published><updated>2025-05-23T00:00:00+05:30</updated><id>http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Community-Bonding-Week-1</id><content type="html" xml:base="http://localhost:4000/gsoc2025-Sakhineti_Praveena/blog/2025/Community-Bonding-Week-1/"><![CDATA[<p>I am excited to be part of GSoC 2025! This week marks the official start of the program with <strong>community bonding period</strong>, and I couldn’t be more thrilled to dive in.</p>

<p>My project, <strong>“Extend DetectionMetrics: GUI, CI Workflow, and Object Detection”</strong>,  will introduce me to perception model evaluation and give me hands on experience in the field of computer vision. <a href="https://github.com/JdeRobot/DetectionMetrics">DetectionMetrics</a> is a toolkit used for evaluating perception models across various frameworks and datasets. Ironically the latest version of Detectionmetrics currently supports image segmentation model evaluations, during my coding period my objective is to extend its functionality to object detection evaluation, build an interactive user interface and set up the CI workflow. This project is a perfect opportunity for me to gain hands-on experience in <strong>computer vision</strong>, and I’m eager to soak in all the learning that comes with it!</p>

<hr />

<h2 id="meeting-with-mentors">Meeting with Mentors</h2>

<p>This week, I had my first meeting with my mentors. We spent time getting to know each other and discussing the initial steps of the project. We also reviewed the program’s logistics and clarified expectations moving forward. Communication was emphasized, with Slack chosen as the primary tool for day-to-day discussions and updates.</p>

<p>The discussion also covered the initial steps for the project, such as diving deep and learning about different perception models, standard frameworks and datasets. I was also asked to set up this blog to document my journey through the project and for it to be easy for my mentors to keep a track of my progress.</p>

<h2 id="this-weeks-to-do">This Week’s To-Do</h2>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Set up a blog website using Jekyll and GitHub Pages</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Read and analyze literature on perception models and datasets</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Write this first blog post documenting progress</li>
</ul>

<hr />

<h2 id="progress-so-far">Progress So Far</h2>

<p>I started the week by solidifying my foundation, I revisited my <strong>undergraduate computer vision coursework</strong>, focusing on:</p>

<ul>
  <li>Key concepts: <strong>Object Detection</strong>, <strong>Instance Segmentation</strong>, and <strong>Semantic Segmentation</strong></li>
  <li>Core frameworks: <strong>PyTorch</strong> and <strong>TensorFlow</strong></li>
  <li>Standard datasets: <strong>COCO</strong>, <strong>Pascal VOC</strong></li>
  <li>Evaluation metrics: <strong>mAP</strong>, <strong>IoU</strong></li>
</ul>

<p>This foundational review has been super helpful in framing the work ahead and getting me ready for the development phase.</p>

<p>Then I moved on to setting up this blog — drawing inspiration from past GSoC contributor blogs, I used <strong>Jekyll</strong> , a static site generator, along with <strong>GitHub Pages</strong> for hosting the website. I plan on uploading a blog every week with the progress, updates and learnings.</p>

<hr />

<p>I’m looking forward to continuing this journey, learning from my mentors, and contributing meaningfully to the open-source community.</p>]]></content><author><name>Sakhineti Praveena</name></author><category term="blog" /><summary type="html"><![CDATA[I am excited to be part of GSoC 2025! This week marks the official start of the program with community bonding period, and I couldn’t be more thrilled to dive in.]]></summary></entry></feed>